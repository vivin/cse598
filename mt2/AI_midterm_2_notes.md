Introduction and Decision Trees
===============================

 - **Unsupervised Learning**: The agent learns patterns in the input even though no explicit feedback is supplied. Most common unsupervised learning-task is clustering: detecting potentially useful clusters of input examples. For example, a taxi agent might gradually develop a concept of "good traffic days" and "bad traffic days" without ever being given labeled examples.
 - **Reinforcement learning**: The agent learns from a series of reinforcements - rewards or punishments. 
 - **Supervised learning**: The agent observes some example input - output pairs and learns a function that maps from input to output. The outputs can come from a teacher who gives the agent information about what the output is. The output can also come from the agent's percepts and the environment ends up being the teacher.

Noise and lack of labels create a continuum between supervised and unsupervised learning. 

Supervised Learning
-------------------

The task of supervised learning is this:

Given a **training set** of *N* example input-output pairs `(x1, y1), (x2, y2),  ..., (xN, yN)`, where each `yj` was generated by an unkknown function `y = f(x)`, discover a function `h` that approximates the true function `f`. 

`x` and `y` can be any value; they need not be numbers. The function `h` is a **hypothesis**. Learning is a search through the space of possible hypotheses for one that will perform well, even on new examples beyond the training set. To measure the accuracy of a hhypothesis we give it a **test set** of examples that are distinct from the training set. We say a hypothesis **generalizes** well if it correctly predicts the value of *y* for novel examples. Sometimes the function `f` is **stochastic** - it is not strictly a function of `x`, and what we have to learn is a **conditional probability** distribution `P(Y | x)`.

**Classification**: When the output `y` is one of a finit set of values (such as `sunny`, `cloudy`, or `rainy`), the learning problem is called **classification**, and is called boolean or binary classification if there are only two values.

**Regression**: When `y` is a number (such as tomorrow's temperature), the learning problem is called **regression**. (Technically, solving a regression problem is finding a conditional expectation or average value of `y`, because the probability that we have found *exactly* the right real-valued number for `y` is 0.)

How do we choose from among multiple, consistent hypotheses? One answer is to prefer the *simplest* hypothesis consistent with the data. This principle is called **Ockham's razor**.  In general there is a tradeoff between complex hypotheses that fit the training data well, and simpler hypotheses that may generalize better (i.e., the question of overfitting). 

We say a learning problem is **realizable** if the hypothesis space contains the true function. Unfortunately we cannot always tell whether a given learning problem is realizable. 

Learning Decision Trees
-----------------------

A **decision tree** represents a function that takes as input a vector of attribute values and returns a "decision" - a single output value. The input and output values can be discrete or continuous. A decision tree reaches its decision by performing a sequence of tests. Each internal node in the tree corresponds to a test of the value of one of the input attributes, `Ai`, and the branches from the node are labled with the possible values of the attribute, `A_i = v_ik`. Each leaf node in the tree specifies a value to be returned by the function. 

Some functions cannot be represented concisely. For example, the majority function, which returns true if and only if omre than half of the inputs are true, requires an exponentially large decision tree. Decision trees are therefore good for some kinds of functions and bad for others. There no one representation that is efficient for all kinds of funtions. For example, consider the set of all boolean functions on n attributes. How many different functions are in this set? This is the number of different truth tables we can write down. A truth table over n attributes has `2^n` rows, one for each combination of values of the attributes. We can consider the answer column of the table as a `2^n`-bit number that defines the function. THerefore there are `2^(2^n)` different functions. 

Finding a minimal decision tree consistent with the training set is NP-hard. Constructing a minimal binary tree with respect to the expected number of tests required for classifying an unseen instance is NP-complete. Even finding the minimal equivalent decision tree for a given decision tree, or building the optimal decision tree from decision tables is known to be NP-hard. (pg 699)

**Inducing decision trees from examples**

An example for a decision tree consists of an `(x, y)` pair where **`x`** is a verctor of values for the input attributes, and `y` is a single Boolean output value. 

It is guided by four cases:

 1. If the remaining examples are all positive (or all neative), then we are done: we can answer *Yes* or *No*. (e.g., see None and Some branches on pg 701). 
 2. If there are some positive and negative examples, then choose the best attribute to split them. (see Hungry being used to split on pg 701).
 3. If there are no examples left, it means that no example has been observed for this combination of attribute values, and we return a default value calculated from the plurality classification of all examples that we used in contructing the node's parent. (passed along in `parent_examples`).
 4. If there are no attributes left, but both positive and negative examples, it means that these examples have exactly the same description, but different classifications. This can happen because there is an error or noise in the data. 

The set of examples is crucial for *constructing* the tree, but do not appear anywhere in the tree itself. 

We can **evaluate** the accuracy of a learning algorithms with a **learning curve**. We split the examples into a training set and test set. We learn a hypothesis *h* with the training set and measure its accuracy with the test set. For example, if we have 100 examples, we start with a training set of size 1 and increase one at a time up to size 99. For each size, we repeat the process of randomly splitting 20 times, and average the results of the 20 trials. 

**Choosing attribute tests**

We need a formal measure of "fairly good" and "really useless". We can do this with the notion of information gain, which is defined in terms of **entropy**. (Entropy formula pg 704. Entropy is the sum of the probability of each value vk of the random variable V multipled by the inverse of the log (base 2) probability of that variable (vk). 

Hence for a boolean variable, it is just `B(q) = -(qlog2q + (1 - q)log2(1-q))`. Since a boolean variable can only have two values. The probability of one value is always 1 minus probability of the other. So in the case of the decision tree, we can look at the entropy of the goal attribute on the whole set. We can simply look at one example, the positive example, since the formula accounts for the negative example as well `(1 - q)`. So we can simply calculate `H(Goal)`, which is just `B(p / (p + n))` (i.e., we see the percentage of how many times the positive example happens in the example set). 

Now testing on a single attribute will only give us part of the bits of `B(p / (p + n))`. In the restaurant example, testing one attribute only gives us part of the information of 1 bit (of entropy; p = n = 6 so B = 1). We can measure exactly how much by looking at the entropy remaining *after* the attribute test. 

If you have an attribute *A* with *d* distinct values, it divides the training set of examples *E* into subses *E1*, ... *Ed*. Each subset *Ek* has *pk* positive examples and *nk* negative examples. If we go along that particular branch, we will need an additional `B(pk/pk + nk))` bits of information. A randomly chosen example from the training set has the *k*th value for the attribute with probability (*pk* + *nk*)/(*p* + *n*), and so the expected entropy remaining after testing attribute A is:

```
Remainder(A) = sum(k from 1 to d, (pk + nk)/(p + n) * B(pk / pk + nk)
```

The **information gain** from the attribute test on A is the expected reduction in entropy:

```
Gain(A) = B(p / p + n) - Remainder(A)
```

**Generalization and overfitting**

An algorithm may accurately predict every case according to the test data, but it may not be general. This is called overfitting. For decision trees, we can use **decision tree pruning** to combat overfitting. Pruning works by eliminating nodes in the decision tree that are not clearly relevant. We start with a full tree, and then look at a test node that has only leaf nodes as descendants. If the test appears to be irrelevant (detecting only noise in the data), we eliminate the test, replacing it with a leaf node. This process is repeated, considering each test with only leaf descendants, until each one has either been pruned or accepted as is. 

How do we detect irrelevance? Suppose we are at a node consisting of *p* positive and *n* negative examples. If the attribute is irrelevant, we would expect that it would split the examples into subsets that each have roughly the same proprtion of positive examples as the whole set, `p / (p + n)`, and so the information gain will be close to zero. Hence information gain is a good clue to irrelevance. How large a gain should we require in order to split on a particular attribute?

To answer that we do a statistical **significance test**. Such a test begins by assuming there is no underlying pattern (the so-called **null hypothesis**). The the actual data are analyzed to calculate to the extent to which they deviate from a perfect absence of pattern. If the degree of deviation is statistically unlikely (usually taken to mean a 5% probability or less), then that is considered to be good evidence for the presence of a significant pattern in the data. This is basically chi-squared.

So we need observed and expected values for each subset (i.e., for every Ek from 1 to d). The expected values would then just be `p * ((pk + nk) / (p + n))` (for positive) and `n * ((pk + nk) / (p + n))` (for negative). The total deviation would be the overall sum for each k of the sum of the positive and negative chi-squared values. (pg 706). This value delta is distributed according to chi-squared with `v - 1` degress of freedom (where `v` is the number of values for the attribute). 

**Minimum depth decision tree**

We want an easy, simple, elegant decision tree that minimizes misclassification errors. The problem is NP-hard (find min decision tree with min error). 

**Broadening the applicability of decision trees**

See pg 707

 - **Missing data** Given a complete decision tree, how should one classify an example that is missing one of the test attributes? Second, how should one modify the information-gain formula, when some examples have unknown values for the attribute? 
 - **Multivalued attributes** How do you split on something like *ExactTime* that has a bunch of different values that are effectively singletons?
 - **Continuous and integer-valued input attributes** Something like *Height* and *Weight* would have an infinite set of possible values. Decision trees usually try to find a **split point** like, *Weight* &gt; 160. 
 - **Continuous-valued output attributes** If we are trying to predict a numerical output value, we need a **regression tree** rather than a classification tree. A regression tree has at each leaf a linear function of some subset of numerical attributes rather than a single value. Learning algorithm must decide when to stop splitting and begin applying linear regression over the attributes.

Evaluating and Choosing the Best Hypothesis
-------------------------------------------

We want to learn a hypothesis that fits the future data best. We make the **stationary assumption**: that there is a probability distribution over examples that remains stationary over time (pg 708). Examples that satisfy these assumptions are called *independent and identically distributed* or **i.i.d.**. This is a definition for our "future data". 

What is "best fit"? First we define the **error rate** of a hypothesis as the proportion of mistakes it makes. That is the number of times `h(x) != y` for an `(x, y)` example. It does not mean, however, that because the hypothesis has a low error rate on the training set, it will generalize well. To get an accurate evaluation of a hypothesis, we need to test it on a set of examples it has not seen yet.

The simplest approach is **holdout cross-validation**. Randomly split the available data into a training set from which the learning algorithm produces `h` and a test set on which the accuracy of `h` is evaluated. This has the disadvantage that it fails to use all of the available data (pg 708). 

Another technique is *k*-**fold cross-validation**. Each example serves double duty - as training data and test data. First we split the data into *k* equal subsets. We then perform *k* rounds of learning; on each round, *1/k* of the data is held out as a test set and the remaining examples are used as training data. The average test set score of the *k* rounds should then be a better estimate than a single score. Popular values for *k* are 5 and 10. This takes us 5-10 times longer to run but is statistically more-accurate. The extreme is *k = n* or **leave-one-out cross-validation** or **LOOCV**.

**Peeking** causes the test results to be invalidated. Users can inadvertently **peek** at the test data. If you select the hypothesis *on the basis of its test set error rate*, you have peeked.

The best way is to lock your test set away and only use it to validate. 

**Model selection: Complexity versus goodness of fit**

(pg 709, 710)

Higher-degree polynomials can fit the training data better, but when the degree is too high, they will overfit and perform poorly on validation data. Choosing the degree of the polynomial is an instance of the problem of **model selection**. You can think of the task of finding the best hypothesis as two tasks: model selection defines the hypothesis space and then **optimization** finds the best hypothesis within that space.

This section talks about selecting models that are parameterizing by *size*.

Regression and Classification with Linear Models
================================================

Applies to the class of linear functions of continuous-valued inputs.

Univariate linear regression
----------------------------

pg(718, 719)

A univariate linear function with input `x` and output `y` has the form:

```
y = w1x + w0
```

Where `w0` and `w1` are real-valued coefficients to be learned. We use the letter `w` because we can think of the coefficients as **weights**; the value of `y` is changed by changing the relative weight of one term to another. We can define **`w`** to be the vector `[w0, w1]` and define the hypothesis function:

```
hw(x) = w1x + w0
```

The task of finding the `hw` that best fits the data is called **linear regression**. To fit a line to the data, all we have to do is find the values of the weights that minimize the empirical loss. We use the L2 loss function for this. Many forms of learning involve adjusting weights to minimize a loss, so it helps to think about a **weight space** - the space defined by all possible settings of the weights. For univariate regression, the weight space is defined by `w0` and `w1` is two-dimensional. Hence we can graph loss as a function of `w0` and `w1` in a three-dimensional plot. We can see that the loss function is **convex** and this is true for *every* linear-regression problem with an L2 loss function and thus implies that there are no local minima. 

Gradient descent
----------------

To go beyond linear models, we need to face the fact that equations defining minimum loss will often have no closed-form solution. Instead, we will face a general optimization search problem in a continuous weight space. We can address these by a hill-climbing algorithm that follows the **gradient** of the function to be optimizied. In this case, because we are trying to minimize the loss, we will use **gradient descent**. We chose any starting point in weight space (a point in the `(w0, w1)` plane) and then move to a neighboring point that is downhill, repeating until we converge on the minimum possible loss.

The parameter *alpha* is usually called the **learning rate** when we are trying to minimze loss in a learning problem. It can be a fixed constant, or it can decay over time as the learning process proceeds. For univariate regression, the loss function is a quadratic function and so the partial derivative will be a linear function. (pg 719, 720)

Learning rule for the weights:

```
w0 = w0 + alpha(y - hw(x))
w1 = w1 + alpha(y - hw(x)) * x
```

This covers only one training example. For N training examples, we want to minimize the sum of the individual losses for each example. The derivative of a sum is the sum of the derivatives and so:

```
w0 = w0 + alpha * sigma(to j, (yj - hw(xj)))
w1 = w1 + alpha * sigma(to j, (yj - hw(xj)) * xj)
```

These updates constitute the **batch gradient descent** learning rule for univariate linear regression. Convergence to a unique global minimum is guaranteed (as long as we pick alpha small enough) but may be very slow. We have to cycle through all the training data for every step, and there may be many steps.

Another possibility is **stochastic gradient descent**, where we consider only a single training point at a time, taking a step after each one using the learning rule (i.e., not the N training examples; the single-example one). It can be used in an online setting, where new data are coming one at a time, or offline, where we cycle through the same data as many times as is necessary, taking a step after considering each single example. It is often faster than batch gradient descent. With a fixed learning rate alpha, however, it does not guarantee convergence; it can oscillate around the minimum without settling down. In some cases, as we see later, a schedule of decreasing learning rates (as in simulated annealing) does guarantee convergence. 

Multivariate linear regression
------------------------------

We can extend the univariate linear-regression to solve **multivariate linear-regression** problems. In the multivariate case, each example **x**j is an n-element vector. Hence:

```
hsw(xj) = w0 = w1xj,1 + ... + wnxj,n = w0 + sigma(to j, wixj,i)
```

The `w0` term (intercept) stands out as different. We can fold it into this equation by inventing a dummy input-attribute, `xj,0` which is always 1. Then `h` is just the dot-product of the weights and the input vector, or the matrix product of the transpose of the weights and the input vector:

```
hsw(xj) = w . xj = wTxj = sigma(to i, wixj,i)
```
THe best verctor of weights, **w_star** minimizes the squared-error loss over the examples:

```
w* = argmin(over w) sigma(to j, L2(yj, w . xj)
```

Gradient descent will reach the (unique) minimum of the loss function. The update function for each weight is

```
wi = wi + alpha * (sum to j, xj,i * (yj - hw(xj)))
```

It is possible to solve this analytically using linear algebra. If **y** is the output vector for training examples, and **X** is the **data matrix** (i.e., a matrix of inputs with one *n*-dimensional example per row). Then the solution:

```
w_star = inverse(transpose(X) * X) * transpose(X) * y
```

With univariate we don't have to worry about overfitting. But with multivariate linear regression in high-dimensional spaces, it is possible that some dimension that is actually irrelevant, appears by chance to be useful, resulting in **overfitting**. Hence, we can use **regularization** on multivariate linear functions to avoid overfitting. The process of explicitly penalizing complex hypothesis is called **regularization**. This is because it looks for a function that is more regular, or less complex. We do this via a cost function:

```
Cost(h) = EmpLoss(h) + lambda * Complexity(h)
```

Hence the best hypothesis is obtained by finding the one with the minimum cost in the entire hypothesis-space.

The choice of regularization function depends on the hypothesis space. For polynomials, a good regularization function is the sum of the squares of the coefficients. Keeping the sum small keeps us away from wiggly polynomials. Another way to simplify models is to reduce the dimensions that models work with. A process of **feature selection** can be performed to discard attributes that appear to be irrelevant. chi-squared pruning is a kind of feature selection.

For linear functions, the complexity can be specified as a function of weights. We can consider a family of regularization functions:

```
Complexity(hw) = Lq(w) = sigma(to i, abs(wi)^q)
```

As with loss functions, with `q = 1` we have L1 regularization, which minimizes the sum of the absolute values. With `q = 2`, we have L2 regularization which minimizes the sum of the squares? When one should you pick? It depends on the problem, but L1 regularization tends to produce a **sparse model**. That is, it often sets many weights to zero, effectively declaring the corresponding attributes to be irrelevant (like how DECISION-TREE-LEARNING does, but DTL does it by a different mechanism). Hypothesis that discard attributes can be easier for a human to understand, and may be less likely to overfit.

(pg 722) Explanation as to why L1 regularization leads to weights of zero, while L2 regularization does not. L1 prioritizes axes, where values of the weights can be zero. We are looking for an intersection between the Lq regularization space and the contours of the minimal achievable loss. At the intersection is where we have a hypothesis that minimizes cost (i.e., is not very complex and minimizes loss). L2 treats dimensional axes as arbitrary. L2 is spherical, which makes it rotationally invariant. L1 is appropriate when the axes are not interchangeable. 

Linear classifiers with a hard threshold
----------------------------------------

Linear functions can be used to do classification as well as regression. (pg 723)

A **decision boundary** is a line (or a surface, in higher dimensions) that separates the two classes. A linear decision boundary is called a **linear separator** and data that admit such a separator are called **linearly separable**.

For the earthquake vs. nuke classification problem, we have the linear classifier `-4.9 + 1.7x1 - x2`. Where it is greater than 0, we have nuclear explosions whereas earthquakes are where this function evaluates to less than 0. If we use the convention of a dummy input such that `x0 = 1`, we can write the classification hypothesis as follows:

```
hw(x) = 1 if w . x &gt;= 0 and 0 otherwise
```

Alternatively, we can think of `h` as the result of passing the linear function `w . x` through a **threshold function**:

```
hw(x) = Threshold(w . x) where Threshold(z) = 1 if z &gt;= 0 and 0 otherwise
```

We don't have a closed-form solution that we can solve, and we cannot use gradient-descent either since the slope is 0 everywhere except at the point of discontinuity, where it does not exist at all (not differentiable). However, we can use a simple weight-update rule that converges to a solution - that is, a linear separator that classifies the data perfectly - provided the data are linearly separable. For a single example (x, y) we have:

```
wi = wi + alpha (y - hw(x)) * xi
```

This is basically the update rule for linear regression, and is also called the **perceptron learning rule**. Since we are considering a 0/1 classification problem, the behavior is somewhat different. Both the true value `y` and the hypothesis output `hw(x)` can be 0 or 1. So there are three cases:

 - If the output is correct, i.e., `y = hw(x)`, then the weights are not changed.
 - If `y` is 1 but `hw(x)` is 0, then `wi` is increased since we want to make `w . x` bigger so that `hw(x)` outputs 1.
 - If `y` is 0 but `hw(x)` is 1, then `wi` is decreased when the corresponding input `xi` is positive and increased when `xi` is negative. This meaks sense because we want to make `w . x` smaller so that `hw(x)` outputs 0.

learning curve etc: (pg 724)

What if the data points are not linearly separable? This happens commonly in the real world. We can show the perceptron learning-rule failing to converge. In general the perceptron rule may not converge to a stable solution for a fixed learning rate. But if the learning rate alpha decays as `O(1 / t)` where `t` is the iteration number, then the rule can be shown to converge to a minimum-error solution when examples are presented in a random sequence. It can also be shown that finding the minimum-error solution is NP-hard, so one expects that many presentations of the examples will be requred for convergence to be achieved. 

Linear classification with logistic regression
----------------------------------------------

In linear classifiers, we saw that passing the output of the linear function through the threshold function creates a linear classifier. Unfortunately the hard nature of the threshold causes some problems. Mainly `hw(x)` is no-longer differentiable since it is discontinuous. Therefore, learning the perceptron rule is very unpredictable. Another problem is that the linear classifier outputs a completely-confident prediction of `1` or `0`, even for examples that are quite close to the boundary. In many situations, we need more gradated predictions.

We can solve this by softening the threshold function, i.e., approximating the hard threshold with a continuous, differentiable function. The function we will use is called the logistic function:

```
Logistic(z) = 1 / (1 + e ^ (-z))
```

Therefore, we now have:

```
hw(x) = Logistic(w . x) = 1 / (1 + e ^ (-w . x))
```

This means the output is a number between 0 and 1 and we can interpret it as a *probability* of belonging to the class labeled 1. The hypothesis forms a soft boundary in the input space and gives a probability of 0.5 for any input at the center of the region, and approaches 0 or 1 as we move away from the boundary.

The process of fitting the weights of this model to minimize loss on a data set is called **logistic regression**. There is no easy closed-form solution to find the optimal value of **w** with this model, but we can still use gradient descent. Our hypothesis no-longer outputs just 0 or 1, so we can use the L2 loss function. (pg 726 for full derivation)

This eventually gives us the weight update rule for minimizing the loss as:

```
wi = wi + alpha (y - hw(x)) * hw(x)(1 - hw(x)) * xi
```

Nonparametric models
====================

Linear regression and neural networks use the training data to estimate a fixed set of parameters **w**. That defines our hypothesis `hw(x)`, and that point we can throw away the training data, because  they are all summarized by **w**. A learning model that summarizes data with a set of parameters of fixed size (independent of the number of training examples, i.e., does not depend on size of knowledge base) is called a **parametric model**.  (pg 737).

It doesn't matter how much data you throw at a parameteric model; it won't change its mind about how many parameters it needs. When data sets are small, it makes sense to have a strong restriction on the allowable hypotheses, to avoid overfitting. But when there are thousands or millions or billions of examples to learn from, it seems like a better idea to let the data speak for themselves rather than forcing them to speak through a tiny vector of parameters. If the data say that the correct answer is a very wiggly function, we shouldn't restrict oursels to linear or slightly wiggly functions.

A **nonparameteric model** is one that cannot be characterized by a bounded set of parameters. For example, suppose that each hypothesis we generate simply retains within itself all of the training examples and uses all of them to predict the next example. Such a hypothesis family would be nonparametric because the effective number of parameters is unbounded - it grows with the number of examples. This approach is called **instance-based learning** or **memory-based learning**. The simplest instance-based learning method is **table lookup**. That is, when asked for `h(x)`, see if `x` is in the table; if it is,return the corresponding `y`. The problem is that it does not generalize well. If `x` is not in the table, it can only return some default value of `y`.

Support Vector Machines
=======================

The **support vector machine** or SVM framework is currently the most-popular approach for "off-the-shelf" supervised learning: if you don't havea ny specialized prior knowledge about a domain, then the SVM is an excellent method to try first. SVMs have three properties that make them attractive:

 1. SVMs construct a **maximum margin separator** - a decision boundary with the largest-possible distance to example points. This helps them generalize well.

 2. SVMs create a linear seprarating hyperplane, but they have the ability to embed the data into a higher-dimensional space, using the so-called **kernel trick**. Often, data that are not linearly separable in the original input space are easily separable in the higher-dimensional space. The high-dimensional linear separator is actually nonlinear in the original space. This means the hypothesis space is greatly expanded over methods that use strictly-linear representations.

 3. SVMs are a nonparametric method - they retain training examples and potentially need to store them all. On the other hand, in practice they often end up retaining only a small fraction of the number of examples - sometimes as few as a small constant times the number of dimensions. Thus SVMs combine the advantages of nonparametric and parametric models: they have the flexibility to represent complex functions, but they are **resistant to overfitting**. 

SVMs are successful because of one key insight and one neat trick. Assume a binary classification problem with three candidate decision boundaries, each a linear seprator. Each of them is consistent with all the examples, so from the point of view of 0/1 loss, each would be good (pg 745 has examples in figure 18.30). Logistic regression would find some separating line, and its location depends on *all* the example points. **The key insight** of SVMs is that that examples are *more important* than others, and that paying attention to them can lead to better generalization. 

If you consider a separating line which comes close to some number of examples, it certainly minimizes loss and classifies all the examples correctly. But it should make us nervous because so many examples are close to the line; it is entirely possible that other examples will turn out to fall on the other side. SVMs address this issue: instead of minimizing expected *empirical loss* on the training data, SVMs attempt to minimize expected *generalization* loss. We don't know where the as-yet-unseen points may fall, but under the probabilistic assumption that they are drawn from the same distribution as the previously-seen examples, there are some arguments from computational learning theory suggesting that we minimize generalization loss by chossing the separator that is farthest away from the examples we have seen so far. We call this separator, the **maximum margin separator**. The **margin** is the area bounded by lines. The distance between these lines is twice the distance from the separator to the nearest example point. 

Traditionally, SVMs use the convention that class labels are +1 and -1, instead of the +1 and 0 we have seen before. Also, where we put the intercept into the weight vector **w** (and a corresponding dummy 1 value into `xj,0`), SVMs do not do that; they keep the intercept as a separate parameter, `b`. Hence the separator is defined as the set of points:

```
{ x : w . x + b = 0 }
```

It is possible to search the space of **`w`** and `b` with gradient descent to find the parameters that maximize the margin while correctly classifying all the examples. However, it turns out there is another approach to solving this problem. The alternate representation is called the dual representation, where the optimal solution is found by solving:

```
argmax(alpha, sum(over j, alpha_j)) - (1/2)(sum(over j, k, alpha_j * alpha_k * yj * yk (xj . xk)))
```

Subject to the constraints `alpha_j &gt;= 0` and `sum(over j, alpha_j * yj) = 0`. This is a **quadratic programming** optimization problem that can be solved by many good software packages.

Once we have found the vector `alpha`, we can get back to **`w`** with the equation `w = sum(over j, alpha_j * xj)`. Or we can stay in the dual representation.

The dual representation has some important properties:

 - The expression is convex; it has a single global maximum that can be found efficiently
 - The data enter the expression only in the form of dot products of pairs of points. This is true of the equation for the separator itself; once the optimal `alpha_j` have been calculated, it is `h(x) = sign(sum(over j, alpha_j * y_j * (x . xj) - b)`
 - The weights `alpha_j` associated with each data point are *zero* except for the **suport vectors** - the points closest to the separator. They are called support vectors because they "hold up" the separating plane. Because there are usually many fewer support vectors than examples, SVMs gain some of the advantages of parametric models. 

What if the examples are *not* linearly separable (pg 746, 747)? We can re-express the data - i.e., we map each input vector **x** to a new vector of feature values, F(**x**). An example:

```
f1 = x1^2
f2 = x2^2
f3 = sqrt(2) * x1 * x1
```

It turns out now that the data in the new, three-dimensional space defined by the three features is *linearly separable* by a plane. If the data are mapped into a space of sufficiently high dimension, then they will almost always be linearly separable. For example, four dimensions suffice for linearly separating a circle anywhere in the plane, and five dimensions suffice to linearly separate any ellipse. In general, if we have N data points, then they will always be separable in spaces of N - 1 dimensions or more.

Now we would not expect to find a linear separator in the input space **x**, but we can find linear separators in the high-dimensional feature space F(**x**) simply by replacing `xj . xk` by `F(xj) . F(xk)`. This is very straightforward since we can replace **x** by F(**x**) in any learning algorithm. But the dot product has special properties. We can often compute `F(xj) . F(xk)` without first comupting F for each point. That is through some simple algebra we can show:

```
F(xj) . F(xk) = (xj . xk)^2
```

That's why the `sqrt(2)` is in `f3`. The expression `(xj . xk)^2` is called a **kernel function** and is usually written as `K(xj, xk)`. Hence, we can learn in the higher dimensional space, but we compute only kernel functions rather than the full list of features for each data point. (pg 747 for additional stuff on kernel functions).

This is the clever **kernel trick**. Plugging these kernels into the equation (the giant equation for SVM), *optimal linear separators can be found efficiently in feature spaces with billions of (or, in some cases, infinitely many) dimensions.* The resulting linear separators, when mapped back to the original input space, can correspond to arbitrarily wiggly, nonlinear decision boundaries between the positive and negative examples.

What to do with noisy data? If the data are inherently noisy, we may not want a linear separator in some high-dimensional space. Rather, we'd like a decision surface in lower-dimensional space that does not cleanly separate the classes, but reflects the reality of the noisy data. That is possible with the **soft margin** classifier, which allows examples to fall on the wrong side of the decision boundary, but assigns them a penalty proportional to the distance required to move them back on the correct side. (pg 748 last para of SVM for other places where kernel function can be applied)

Ensemble Learning
=================

Previous learning methods employ a single hypothesis, chosen from a hypothesis space, to make predictions. The idea of **ensemble learning** methods is to select a collection, or **ensemble** of hypotheses from the hypothesis space and combine their predictions. As an example, during cross-validation we might generate twenty different decision trees, and have them vote on the best classification for a new example.

The motivation for ensemble learning is simple. Consider an ensemble of K = 5 hypotheses and suppose that we combine their predictions using simple majority voting. For the ensemble to misclassify a new example, *at least three of the five hypotheses have to misclassify it*. The hope is that this is much less likely than a misclassification by a single hypothesis. The assumption here is that the errors are independent. 

Another way to view the ensemble idea is to think of it as a generic way of enlarging the hypothesis space. Think of the ensemble itself as a hypothesis, and the new hypothesis space as the set of all possible ensembles constructable from hypotheses in the original space.

Boosting
--------

The most widely used ensemble method is called **boosting**. First we need to understand what a **weighted training set** is. In such a training set, each example has an associated weight `wj &gt;= 0`. The higher the weight of an example, the higher is the importance attached to it during the learning of a hypothesis. We can easily modify existing algorithms to take the weight into account (where this is not possible, you can create a **replicated training set** where the jth example appears `wj` times, using randomization to handle fractional weights). Boosting starts with `wj = 1` for all the examples (normal training set). From this, it generates the first hypothesis `h1`. This hypothesis will classify some of the training examples correctly and some incorrectly. We want the  next hypothesis to do better on the miscalssified examples, so we increase their weights while decreasing the weights of the correctly classified examplles. From this weighted training set, we generate the hypothesis `h2`. The process continues until we have generated *K* hypotheses. The final ensemble hypothesis is a weighted-majority combination of all the K hypothesis, each weighted according to how well it performed on the training set (pg 750 for a visual example). 

A specific algorithm called ADABOOST has a very important property: if the input learning algorithm *L* is a **weak learning** algorithm - which means that *L* always returns a hypothesis with accuracy on the training set that is slightly better than random guessing (i.e., 50% + epsilon for Boolean classification) - then ADABOOST will return a hypothesis that *classifies the training data perfectly* for large enough K. Thus, the algorithm *boosts* the accuracy of the original learning algorithm on the training data. This result holds no matter how inexpressive the original hypothesis space and no matter how complex the function being learned. For decision trees, we can employ boosting on the **decision stumps**, which aredecision trees with just one test, at the root. (pg 750 for details).

The finding of the performance of boosting is a surprise since Ockham's razor tells us not to make hypotheses any more complex than necessary. But here we can see that the prediction improves as the ensemble hypothesis gets more complex. There are many explanations. One view is that boosting approximates **Bayesian learning**, which can be shown to be an optimal learning algorithm, and the approximation improves as more hypotheses are added. Another possible explanation is that the addition of further hypotheses enables the ensemble to be *more definite* in its disctinction between positive and negative examples, which helps it when it comes to classifying new examples.

Bagging
-------

**Bagging** is the first effective method with ensemble learning for improving the performance of learning algorithms. It combines hypotheses learned from multiple **bootstrap** data sets, each generated by subsampling (random) the original data set. 

Here we are given a training set of N examples, and a class of learning models (e.g. decision trees, NN, etc.). We train multiple (k) models on different samples (data splits) and average their predictions. Prediction is done by averaging the results of k models. The goal is to improve the accuracy of one model by using its multiple copies. The average of misclassification errors on different data splits gives a better estimate of the predictive ability of a learning method. For regression we average, for classification we use a majority vote. (see [here](http://people.cs.pitt.edu/~milos/courses/cs2750-Spring04/lectures/class23.pdf) for more information)

Precision and Recall
--------------------

 - Precision is the number of correctly classified instances for class X divided by the total number of instances the algorithm classified for class X.
 - Recall is the number of correctly classified instances for class X divided by the total number of instances of class X in the *test data*. 

Another way: recall is a measure of how many of the relevant documents were retrieved, while precision is a measure of how many of the retrieved documents were relevant.

Given,

 - D = number of documents retrived.
 - R = number of relevant documents retrieved
 - N = number of relevant documents in the collection.

 - Recall = R / N
 - Precision = R / D

Let's have an exacmple where we have 100 documents, 30 of which are relevant to our query. 

 - Algorithm #1 retrieves all 100. So Recall = 100%, and Precision = 30%. So it did retrieve all the documents, but also a bunch of irrelevant ones. A person has to manually inspect these documents.
 - Algorithm #2 retrieves 70 documents, including all 30 revelant documents. Recall = 100%, Precision = 70%. This is a little better. 
 - Algorithm #3 retrieves 60 documents, including 20 relevant. Recall = 20 / 30 = 67%, Precision = 20 / 50 = 40%. 

Another one:

 - P = N(relevant items retrieved) / N(total retrieved) = P(relevant | retrieved)
 - R = N(relevant items retrieved) / N(total relevant) = P(retrieved | relevant)

Which one is important depends on the circumstances. A typical web surfer looking for documents would like every result on the first page to be relevant (high precision) but not have the interest in knowing, let alone looking at every document that is relevant.

In contrast, various professional searchers (paralegals, intelligence analysts) are concerned with trying to get as high recall as possible, and will tolerate fairly low precision results in order to get it. 

You can always get a recall of 1 (but very low precision) by retrieving all documents for all queries. Recall is a non-decreasing function of the number of documents retrieved.
On the other hand, precision usually decreases as the number of documents retrieved is increased.

Probability Theory
==================

Agents need to handle **uncertainty**, whether due to partial observability, nondeterminism, or a combination of the two. We can use probability theory for this.

The set of all possible worlds is the **sample space**. OMEGA (uppercase omega) is used to refer to the sample space, and omega (lowercase omega) refers to the elements of the space.

A fully specified **probability model** associates a numerical probability *P*(omega) with each possible world. The total probability of the set of possible worlds is 1:

```
0 &lt;= P(omega) &lt;= 1 for every omega and sum(for every omega in OMEGA, P(omega)) = 1
```

Probabilistic assertions and queries are not usually about particular possible worlds, but sets of them. For example, we might be interested in the cases where two dice add up to 11. In probability theory, these sets are called **events**. In AI, the sets are always described by **propositions** in a formal language. The probability associated with a proposition is defined to be the sum of pobabilities of the worlds in which it holds:

```
For any proposition PHI, P(PHI) = sum(for every omega in PHI, P(omega))
```

Probabilities such as *P*(*Total* = 11) and *P*(*doubles*) are called **unconditional** or **prior probabilities**. In other cases we are interested in the **conditional** or **posterior** probability. For example *P*(*doubles* | *Die1* = 5). Conditional probabilities are defined in terms of unconditional probabilities as follows:

**Definition of Conditional Probability**:
```
P(a | b) = P(a AND b) / P(b)
```

**Conditional Probability in the form of the Product Rule**
```
P(a AND b) = P(a | b)P(b)
```

Variables in probability theory are called **random variables** and their names begin with an uppercase letter. Every random variable has a **domain** - the set of possible values it can take on. For example, a Boolean random variable can take the values `{true, false}`. The proposition that doubles are rolled can be written as `Doubles = true`.

By convention, propositions of the form `A = true` are simply abbreviated as `a` whereas `A = false` is abbreviated as `NOT a`. 

Variables can have infinite domains as well. For any variable with an ordered domain, inequalities are allowed as well such as NumberOfAtomsInUniverse &gt;= 10^70.

Sometimes we will want to talk about the probabilities of all the possible values of a random variable. We could write:

 - *P*(Weather = sunny) = 0.6
 - *P*(Weather = rain) = 0.1
 - *P*(Weather = cloudy) = 0.29
 - *P*(Weather = snow) = 0.01

But as an abbreviation, we can have:

 - **P**(Weather) = &lt;0.6, 0.1, 0.29, 0.01&gt;

The bold **P** indicates that the result is a vector of numbers, where we assume a pre-defined ordering on the domain of *Weather*. We say that the **P** statement defines a **probability distribution** for the random variable *Weather*. The **P** notation is also used for conditional distributions: **P**(*X* | *Y*) gives the values of *P*(*X* = *xi* | *Y* = *yi*) for each possible *i*, *j* pair.

For continuous variables, it is not possible to write out the entire distribution as a vector, because there are infinitely many values. Instead, we can define the probability that a random variable takes on some value *x* as a parameterizing function of *x*. For example:

*P*(*NoonTemp* = *x*) = *Uniform*\[*18C*, *26C*\](*x*)

expresses the belief that the temperature at noon is distributed uniformly between 18 and 26. We call this function a **probability density function**.

Saying that the probability density is uniform from 18C to 26C means that there is a 100% chance that the temperature will fall somewhere in that 8C-wide region and a 50% chance that it will fall in any 4C-wide region, and so on. (pg 487)

In addition to distributions on single variables, we need notation for distributions on multiple cariables. Commas are used for this. For example, **P**(*Weather*, *Cavity*) denotes the probabilities of all combinations of the values of *Weather* and *Cavity*. This is called the **joint probability distribution**. We can mix variables with and without values; **P**(*sunny*, *Cavity*) would be a two-element vector giving the probabilities of a sunny day with a cavity and a sunny day with no cavity. The **P** notation makes certain expressions much more concise that they otherwise might be. For example, the product rules for all possible values of *Weather* and *Cavity* can be written as a single equation:

**P**(*Weather*, *Cavity*) = **P**(*Weather* | *Cavity*)**P**(*Cavity*)

This is much more concise than writing 4 x 2 = 8 equations. 

As a degenerate case, **P**(*sunny*, *cavity*) has no variables and thus is a one-element vector that is the probability of a sunny day with a cavity, which could also be written as *P*(*sunny*, *cavity*) or *P*(*sunny* AND *cavity*). We will sometimes use **P** notation to derive results about invidividual *P* values, and when we say **P**(*sunny*) = 0.6, it is really an abbreviation for "**P**(*sunny*) is the one-element vector &lt;0.6&gt;, which means that *P*(*sunny*) = 0.6."

A probability model is completely determined by the joint distribution for all of the random variables - this is the **full joint probability distribution**. For example, if the random variables are *Cavity*, *Toothache*, and *Weather*, then the full joint distribution is given by **P**(*Cavity*, *Toothache*, *Weather*). 

Probability axioms and their reasonableness
-------------------------------------------

**Inclusion-exclusion principle**
```
P(a OR b) = P(a) + P(b) - P(a AND b)
```

Inference Using Full Joint Distributions
----------------------------------------

The method of **probabilistic inference** is the computation of posterior probabilities for query propositions given observed evidence. 

A particularly common task is to extract the distribution over some subset of variables or a single variable. For example, adding the entries (pg 492 fig 13.3) in the first row gives the unconditional or **marginal probability** of *cavity*:

*P*(*cavity*) = 0.108 + 0.012 + 0.072 + 0.008 = 0.2

This process is called **marginalization** or **summing out**, because we sum up the probabilities for each possible value of the other variables, thereby taking them out of the equation. We can write the following general marginalization rule for any sets of variables **Y** and **Z**:

**P**(**Y**) = sum(for every **z** in **Z**, **P**(**Y**, **z**))

In our example, we basically did:

**P**(*Cavity*) = sum(for every **z** in {*Catch*, *Toothache*}, **P**(*Cavity*, **z**))

A variant of this rule involves conditional probabilities instead of joint probabilities, using the product rule:

**P**(**Y**) = sum(over **z**, **P**(**Y** | **z**)*P*(**z**))

("over **z**" is just shorthand for "for every **z** in **Z**")

This rule is called **conditioning**. Marginalization and conditioning turn out to be useful rules for all kinds of derivations involving probability expressions.

Conditional probabilities can be found by first using the equation:

*P*(*a* | *b*) = *P*(*a* AND *b*) / *P*(*b*)

For **P**(*Cavity* | *toothache*), we basically have *P*(*cavity* | *toothache*) + *P*(NOT *cavity* | *toothache*). Both will have as denominator *P*(*toothache*), which remains constant no matter what value of *Cavity* we calculate. This means it ends up being a **normalization** constant for the distribution **P**(*Cavity* | *toothache*), ensuring that it adds up to 1. We will use *alpha* to denote such constants. Hence we can write:

**P**(*Cavity* | *toothache*) = *alpha* * **P**(*Cavity*, *toothache*) = *alpha* * \[**P**(*Cavity*, *toothache*, *catch*) + **P**(*Cavity*, *toothache*, NOT *catch*)\]

We end up with *alpha* * &lt;0.12, 0.08&gt; = &lt;0.6, 0.6&gt;.

Here, even if we don't know *P*(*toothache*), we can forget about 1/*P*(*toothache*) and just add up the values getting 0.12 and 0.08. The relative proprtions are right and so we can nromalize by dividing each of them by 0.12 + 0.08, which gives us the correct probabilities.

We can now extract a general inference procedure. If we have a query with a single variable *X* (like *Cavity*), let **E** be the list of evidence variables (just *Toothache* in the example), and let **e** be the observed values for them, and let **Y** be the remaining unobserved variables (just *Catch* in the example). If the query is **P**(*X* | **e**), we can evaluate it as:

**P**(*X* | **e**) = *alpha* * **P**(*X*, **e**) = *alpha* * sum(over **y**, **P**(*X*, **e**, **y**))

where the summation is over all possible **y**s (i.e., all possible combinations of values of the unobserved variables **Y**). Notice that *X*, **E**, and **Y** constitute the complete set of variables for the domain and so **P**(*X*, **e**, **y**) is simply a subset of probabilities from the full joint distribution.

Given a full joint distribution, we can answer probabilistic queries with this equation but it does not scale well. Consider a domain with *n* boolean variables. It needs an input table of size *O*(2^*n*) and takes *O*(2^*n*) time to process the table. 

Independence
------------

What if we add another variable, *Weather*, to the toothache table? The full joint distribution is now **P**(*Toothache*, *Catch*, *Cavity*, *Weather*). What relationship can we infer? For example, how are *P*(*toothache*, *catch*, *cavity*, *cloudy*) and *P*(*toothache*, *catch*, *cavity*) related? Using the product rule:

*P*(*toothache*, *catch*, *cavity*, *cloudy*) = *P*(*cloudy* | *toothache*, *catch*, *cavity*)*P*(*toothache*, *catch*, *cavity*)

The weather has no bearing on the dental variables, so we can say:

*P*(*cloudy* | *toothache*, *catch*, *cavity*) = *P*(*cloudy*)

From this, we can deduce:

*P*(*toothache*, *catch*, *cavity*, *cloudy*) = *P*(*cloudy*)*P*(*toothache*, *catch*, *cavity*)

A similar equation exists for every entry in **P**(*Toothache*, *Catch*, *Cavity*, *Weather*) and therefore we can write the general equation:

**P**(*Toothache*, *Catch*, *Cavity*, *Weather*) = **P**(*Toothache*, *Catch*, *Cavity*)**P**(*Weather*)

This property is called **independence** (also **marginal independence** and **absolute independence**). In particular, the weather is independent of one's deptal problems. Independence between propositions *a* and *b* can be written as:

*P*(*a* | *b*) = *P*(*a*) or *P*(*b* | *a*) = *P*(*b*) or *P*(*a* AND *b*) = *P*(*a*)*P*(*b*)

Independence between variables *X* and *Y* can be written as follows:

**P**(*X* | *Y*) = **P**(*X*) or **P**(*Y* | *X*) = **P**(*Y*) or **P**(*X*, *Y*) = **P**(*X*)**P**(*Y*)

Bayes' Rule and its Use
=======================

Bayes rule is:

*P*(*b* | *a*) = *P*(*a* | *b*)*P*(*b*) / *P*(*a*)

In the more general case, for multivalued variables:

**P**(*Y* | *X*) = **P**(*X* | *Y*)**P**(*Y*) / **P**(*X*)

We also have a more general version conditionalized on some background evidence **e**:

**P**(*Y* | *X*, **e**) = **P**(*X* | *Y*, **e**)**P**(*Y* | **e**) / **P**(*X* | **e**)

Applying Bayes' rule: The simple case
-------------------------------------

Often we perceive as evidence the *effect* of some unknown *cause* and we would like to determine that cause. In that case, Bayes' rule becomes:

*P*(*cause* | *effect*) = P(*effect* | *cause*)P(*cause*) / *P*(*effect*)

The conditional probably *P*(*effect* | *cause*) quantifies the relationship in the **causal** direction, whereas *P*(*cause* | *effect*) describes the **diagnostic** direction. In medical diagnosis, we often have conditional probabilities on causal relationships. For example, the doctor knows *P*(*symptoms* | *disease*) and want to derive a diagnosis *P*(*disease* | *symptoms*). 

For example, what if the doctor wants to know if the disease is meningitis given some symptoms? 

Let's say:

 - *P*(*stiff neck* | *meningitis*) = 0.7
 - *P*(*meningitis*) = 1/50000
 - *P*(*stiff neck*) = 0.01

Then *P*(*m* | *s*) = *P*(*s* | *m*)*P*(*m*) / *P*(*s*) = (0.7 * (1/50000)/0.01 = 0.0014

Recall that we can avoid accessing the prior probability of the evidence (*P*(*s*) here) by instead computing a posterior probability for each value of the query variable (here *m* and *not m*) and then normalizing the results. We can apply the same process using Bayes rule:

**P**(*M* | *s*) = *alpha* &lt;**P**(*s* | *m*)*P*(*m*), *P*(*s* | NOT *m*)*P*(NOT *m*)&gt;

To use this approach, we need to estimate *P*(*s* | NOT *m*) instead of *P*(*s*). This information may be easier to obtain than *P*(*s*) or it may be harder. The idea is that we have an alternate representation that we can use depending on how difficult it is to get some information. The general form of the Bayes' rule with normalization is:

**P**(*Y* | *X*) = *alpha* * **P**(*X* | *Y*)**P**(*Y*)

Using Bayes' rule: Combining evidence
-------------------------------------

What if we want to answer questions based on multiple pieces of evidence?

For example, what if we wnat to answer **P**(*Cavity* | *toothache* AND *catch*)? We know that this approach does not scale up to large numbers of variables. We can try using Bayes' rule to reformulate it:

**P**(*Cavity* | *toothache* AND *catch*) = *alpha* * **P**(*toothache* AND *catch* | *Cavity*)**P**(*Cavity*)

For this to work we need conditional probabilities of *toothcahe* AND *catch* for each value of *Cavity*. However, this does not scale up. So what do we do? We can refine the earlier notion of **independence** to get the notion of **conditional independence**. 

For example, it would be nice if *Toothache* and *Catch* were independent. They are not, because if the probe catches in the tooth, then it is likely that the tooth has cavity, and the cavity causes a toothache. However, these variables *are* independent *given the presence or absence of a cavity*. Each is directly caused by a cavity, but neither has a direct effect on the other (toothache is caused by the nerves signalling pain, and a catch is caused by the dentist's tool catching on the tooth). 

Hence, we can say:

**P**(*toothache* AND *catch* | *Cavity*) = **P**(*toothache* | *Cavity*)**P**(*catch* | *Cavity*)

This equation expresses the **conditional independence** of *toothache* and *catch* given *Cavity*. This means we can plug it into the earlier equation)

**P**(*Cavity* | *toothache* AND *catch*) = *alpha* * **P**(*toothache* | *Cavity*)**P**(*catch* | *Cavity*)**P**(*Cavity*)

The general definition of **conditional independence** of two variables *X* and *Y* given a third variable *Z*, is

**P**(*X*, *Y* | *Z*) = **P**(*X* | *Z*)**P**(*Y* | *Z*)

Hence:

**P**(*Tootache*, *Catch* | *Cavity*) = **P**(*Toothache* | *Cavity*)**P**(*Catch* | *Cavity*)

As with absolute independence, we can have the equivalent forms:

**P**(*X* | *Y*, *Z*) = **P**(*X* | *Z*) and **P**(*Y* | *X*, *Z*) = **P**(*Y* | *Z*)

This means you can decompose the full joint distrubtion into smaller pieces.

For example:

**P**(*Toothache*, *Catch*, *Cavity*) = **P**(*Toothache*, *Catch* | *Cavity*)**P**(*Cavity*) (product rule) = **P**(*Toothache* | *Cavity*)**P**(*Catch* | *Cavity*)**P**(*Cavity*)

The idea is that for *n* symptoms that are all conditionally independent given *Cavity*, the size of the representation grows as *O*(*n*) instead of *O*(2^*n*).

Conditional independence assertions can allow probabilistic systems to scale up; morever, they are much more commonly available than absolute independence assertions. (pg 499 for why this helps - more details). The decomposition of large probabilistic domains into weakly connected subsets through conditional independence is one of the most important developments in the recent history of AI. The dentistry example illustrates a commonly occurring pattern in which a single cause directly influences a number of effects, all of which are conditionally independent, given the cause. The full joint distribution can be written as:

**P**(*Cause*, *Effect 1*, ..., *Effect N*) = **P**(*Cause*) * product(over *i*, **P**(*Effect i* | *Cause*)

Such a probability distribution is called a **naive Bayes** model. This is sometimes called a **Bayesian classifier**, which has prompted true Bayesians to call it the **idiot Bayes** model. 

The Bayes classifier is the function that assigns a class label *y = Ck* for some *k* as follows:

*y* = argmax(for *k* in *1..K*)(*P*(*Ck*)product(*i* to *n*, *P*(*xi*|*Ck*)))

![Bayes classifier](http://upload.wikimedia.org/math/3/5/e/35e94f179a666c4b5892a11de1b3b29e.png)

